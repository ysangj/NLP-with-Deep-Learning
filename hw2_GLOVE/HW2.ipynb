{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sec 2: Building GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to run GloVe on the text of the Stanford Sentiment Treebank (SST) training set. Usually these methods are run on extremely large corpora, but we're using this here to make sure that you can train a reasonable model without waiting for hours or days. \n",
    "\n",
    "First, let's load the data as before. For our purposes, we won't need either the labels nor any of the test and dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sst_home = 'data/trees' #..'/data/trees'\n",
    "\n",
    "import re\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "    return data\n",
    "training_set = load_sst_data(sst_home + '/train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For speed, we're only using the 250 most common words. Extract cooccurence counts from the corpus,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1550 times where 'the' is both target word and context word.\n",
      "[[   14.  1074.   624. ...,    11.     9.    19.]\n",
      " [ 1074.  1550.  2043. ...,    16.    13.    19.]\n",
      " [  624.  2043.  2262. ...,    19.    28.    36.]\n",
      " ..., \n",
      " [   11.    16.    19. ...,     0.     0.     0.]\n",
      " [    9.    13.    28. ...,     0.     0.     0.]\n",
      " [   19.    19.    36. ...,     0.     0.     0.]]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "context_window = 4\n",
    "top_k = 250\n",
    "\n",
    "def tokenize(string):\n",
    "    string = string.lower()\n",
    "    return string.split()\n",
    "\n",
    "word_counter = collections.Counter()\n",
    "for example in training_set:\n",
    "    word_counter.update(tokenize(example['text']))\n",
    "vocabulary = [pair[0] for pair in word_counter.most_common()[0:top_k]]\n",
    "index_to_word_map = dict(enumerate(vocabulary))\n",
    "word_to_index_map = dict([(index_to_word_map[index], index) for index in index_to_word_map])\n",
    "\n",
    "def extract_cooccurrences(dataset, word_map, amount_of_context=context_window):\n",
    "    num_words = len(vocabulary)\n",
    "    cooccurrences = np.zeros((num_words, num_words))\n",
    "    nonzero_pairs = set()\n",
    "    checking_word = 'the' \n",
    "    checker = 0\n",
    "    for example in dataset:\n",
    "        words = tokenize(example['text'])\n",
    "        for target_index in range(len(words)):\n",
    "            target_word = words[target_index]\n",
    "            if target_word not in word_to_index_map:\n",
    "                continue\n",
    "            target_word_index = word_to_index_map[target_word]\n",
    "            min_context_index = max(0, target_index - amount_of_context)\n",
    "            max_word = min(len(words), target_index + amount_of_context + 1)\n",
    "            for context_index in list(range(min_context_index, target_index)) + \\\n",
    "            list(range(target_index + 1, max_word)):\n",
    "                context_word = words[context_index]\n",
    "                if context_word not in word_to_index_map:\n",
    "                    continue\n",
    "                if(checking_word == context_word and checking_word == target_word):\n",
    "                    checker = checker + 1\n",
    "                    \n",
    "                context_word_index = word_to_index_map[context_word]\n",
    "                cooccurrences[target_word_index][context_word_index] += 1.0\n",
    "                nonzero_pairs.add((target_word_index, context_word_index))\n",
    "    print('There are', checker ,'times where \\'the\\' is both target word and context word.')\n",
    "    return cooccurrences, list(nonzero_pairs)\n",
    "                \n",
    "cooccurrences, nonzero_pairs = extract_cooccurrences(training_set, vocabulary)\n",
    "print(cooccurrences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def  batch_iter(nonzero_pairs, cooccurrences, batch_size):\n",
    "    start = -1 * batch_size\n",
    "    dataset_size = len(nonzero_pairs)\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        word_i = []\n",
    "        word_j = []\n",
    "        counts = []\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [nonzero_pairs[index] for index in batch_indices]\n",
    "        for k in batch:\n",
    "            counts.append(cooccurrences[k])\n",
    "            word_i.append(k[0])\n",
    "            word_j.append(k[1])\n",
    "        yield [counts, word_i, word_j]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Evalation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be frank, a GloVe model trained on such a small dataset and vocabulary won't be spectacular, so we won't bother with a full-fledged similarity or analogy evaluation. Instead, we'll use the simple scoring function below, which grades the model on how well it captures ten easy/simple similarity comparisons. The function returns a score between 0 and 10. Random embeddings can be expected to get a score of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity(model, word_one, word_two):\n",
    "    vec_one = model.get_embeddings(word_to_index_map[word_one]).reshape(1, -1)\n",
    "    vec_two = model.get_embeddings(word_to_index_map[word_two]).reshape(1, -1)\n",
    "    return float(cosine_similarity(vec_one, vec_two))\n",
    "\n",
    "def score(model):\n",
    "    m = model\n",
    "    score = 0\n",
    "    score += similarity(m, 'a', 'an') > similarity(m, 'a', 'documentary')\n",
    "    score += similarity(m, 'in', 'of') > similarity(m, 'in', 'picture')\n",
    "    score += similarity(m, 'action', 'thriller') >  similarity(m, 'action', 'end')\n",
    "    score += similarity(m, 'films', 'movies') > similarity(m, 'films', 'good')\n",
    "    score += similarity(m, 'film', 'movie') > similarity(m, 'film', 'movies')\n",
    "    score += similarity(m, 'script', 'plot') > similarity(m, 'script', 'dialogue')\n",
    "    score += similarity(m, 'character', 'human') > similarity(m, 'character', 'young')\n",
    "    score += similarity(m, '``', \"''\") > similarity(m, '``', 'quite')\n",
    "    score += similarity(m, 'funny', 'entertaining') > similarity(m, 'funny', 'while')\n",
    "    score += similarity(m, 'good', 'great') > similarity(m, 'good', 'minutes')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've built and trained the model, you can evaluate it by calling `score(model)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Implement and train\n",
    "\n",
    "There's some starter code below for training a PyTorch model. **Fill it out to create an implementation of GloVe, then train it on the SST training set.**\n",
    "Try not to modify any of the starter code.\n",
    "\n",
    "### 2.3.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Glove(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, batch_size):\n",
    "        super(Glove, self).__init__()        \n",
    "        self.word_embeddings = None\n",
    "        \n",
    "        self.W = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.W_Tilda = nn.Embedding(vocab_size, embedding_dim)\n",
    "       \n",
    "        self.Bias = nn.Embedding(vocab_size, 1)\n",
    "        self.Bias_Tilda = nn.Embedding(vocab_size, 1)\n",
    "    \n",
    "    def forward(self, words_var, co_words_var):\n",
    "        w = self.W(words_var)\n",
    "        w_tilda = self.W_Tilda(co_words_var)\n",
    "        \n",
    "        w_bias = self.Bias(words_var)\n",
    "        w_tilda_bias = self.Bias_Tilda(co_words_var)\n",
    "        \n",
    "        matmul = (w*w_tilda).sum(1).view(batch_size, 1)\n",
    "        res = (matmul + w_bias + w_tilda_bias).squeeze(1)\n",
    "        \n",
    "        return res\n",
    "                \n",
    "    def init_weights(self):\n",
    "        self.W.weight = nn.init.xavier_normal(self.W.weight)\n",
    "        self.W_Tilda.weight = nn.init.xavier_normal(self.W_Tilda.weight)\n",
    "        self.Bias.weight = nn.init.xavier_normal(self.Bias.weight)\n",
    "        self.Bias_Tilda.weight = nn.init.xavier_normal(self.Bias_Tilda.weight)\n",
    "\n",
    "    \n",
    "    def add_embeddings(self):\n",
    "        self.word_embeddings = self.W.weight.data.cpu().numpy() + self.W_Tilda.weight.data.cpu().numpy()\n",
    "        return self.word_embeddings\n",
    "\n",
    "    \n",
    "    def get_embeddings(self, index):\n",
    "        if self.word_embeddings is None:\n",
    "            self.add_embeddings()\n",
    "        return self.word_embeddings[index, :]\n",
    "    \n",
    "    def weight_function(self, x, xmax, alpha):\n",
    "        if(xmax > x):\n",
    "            return (x/xmax)**alpha\n",
    "        return 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_function(x, xmax, alpha):\n",
    "        if(xmax > x):\n",
    "            return (x/xmax)**alpha\n",
    "        return 1\n",
    "    \n",
    "def training_loop(batch_size, num_epochs, model, optim, data_iter, xmax, alpha):\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "    losses = []\n",
    "    total_batches = int(len(training_set) / batch_size)\n",
    "    while epoch <= num_epochs:\n",
    "        model.train()\n",
    "        counts, words, co_words = next(data_iter)\n",
    "        words_var = Variable(torch.LongTensor(words))\n",
    "        co_words_var = Variable(torch.LongTensor(co_words))\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        FX = Variable(torch.FloatTensor([weight_function(x, xmax, alpha) for x in counts]))\n",
    "        \n",
    "        output = model(words_var, co_words_var)\n",
    "        counts = Variable(torch.FloatTensor(counts))\n",
    "\n",
    "        loss = (torch.pow(output - torch.log(counts), 2) * FX ).sum()        \n",
    "        \n",
    "        losses.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % total_batches == 0:\n",
    "            epoch += 1\n",
    "            if epoch % 25 == 0:\n",
    "                word_emebddings = model.add_embeddings()\n",
    "                print( \"Epoch:\", (epoch), \"Avg Loss:\", np.mean(losses)/(total_batches*epoch), \"Score:\", score(model) )\n",
    "        \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Train a working model\n",
    "\n",
    "\n",
    "You should use the following commands to train the model for at least 2000 steps. If your model works, it will converge to a score of 10 within that many steps. You need to complete the previous code block before you can run this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Avg Loss: 6.38913220341 Score: 6\n",
      "Epoch: 50 Avg Loss: 1.8667274562 Score: 6\n",
      "Epoch: 75 Avg Loss: 0.883304328297 Score: 6\n",
      "Epoch: 100 Avg Loss: 0.525235645527 Score: 8\n",
      "Epoch: 125 Avg Loss: 0.353112612545 Score: 8\n",
      "Epoch: 150 Avg Loss: 0.256390751876 Score: 9\n",
      "Epoch: 175 Avg Loss: 0.196125697978 Score: 9\n",
      "Epoch: 200 Avg Loss: 0.155805405053 Score: 9\n",
      "Epoch: 225 Avg Loss: 0.127372062505 Score: 9\n",
      "Epoch: 250 Avg Loss: 0.106499962278 Score: 9\n",
      "Epoch: 275 Avg Loss: 0.0906589117937 Score: 9\n",
      "Epoch: 300 Avg Loss: 0.0783246091947 Score: 9\n",
      "Epoch: 325 Avg Loss: 0.0685271934455 Score: 9\n",
      "Epoch: 350 Avg Loss: 0.0605862688319 Score: 9\n",
      "Epoch: 375 Avg Loss: 0.0540548775309 Score: 9\n",
      "Epoch: 400 Avg Loss: 0.0486086852096 Score: 9\n",
      "Epoch: 425 Avg Loss: 0.0440152045486 Score: 9\n",
      "Epoch: 450 Avg Loss: 0.0401011218199 Score: 9\n",
      "Epoch: 475 Avg Loss: 0.0367369222969 Score: 9\n",
      "Epoch: 500 Avg Loss: 0.0338190989548 Score: 9\n",
      "Epoch: 525 Avg Loss: 0.0312694676061 Score: 9\n",
      "Epoch: 550 Avg Loss: 0.0290293667513 Score: 9\n",
      "Epoch: 575 Avg Loss: 0.0270484437747 Score: 9\n",
      "Epoch: 600 Avg Loss: 0.0252851547745 Score: 9\n",
      "Epoch: 625 Avg Loss: 0.0237093107985 Score: 10\n",
      "Epoch: 650 Avg Loss: 0.0222930276513 Score: 10\n",
      "Epoch: 675 Avg Loss: 0.0210178186068 Score: 9\n",
      "Epoch: 700 Avg Loss: 0.0198626403131 Score: 9\n",
      "Epoch: 725 Avg Loss: 0.0188108906085 Score: 9\n",
      "Epoch: 750 Avg Loss: 0.017853869883 Score: 9\n",
      "Epoch: 775 Avg Loss: 0.0169758359355 Score: 9\n",
      "Epoch: 800 Avg Loss: 0.0161710919064 Score: 9\n",
      "Epoch: 825 Avg Loss: 0.0154310075191 Score: 9\n",
      "Epoch: 850 Avg Loss: 0.0147483706084 Score: 9\n",
      "Epoch: 875 Avg Loss: 0.014115734663 Score: 9\n",
      "Epoch: 900 Avg Loss: 0.0135297960351 Score: 9\n",
      "Epoch: 925 Avg Loss: 0.0129839642899 Score: 9\n",
      "Epoch: 950 Avg Loss: 0.0124761673172 Score: 9\n",
      "Epoch: 975 Avg Loss: 0.0120026087308 Score: 9\n",
      "Epoch: 1000 Avg Loss: 0.0115594952731 Score: 9\n",
      "Epoch: 1025 Avg Loss: 0.0111441896939 Score: 9\n",
      "Epoch: 1050 Avg Loss: 0.0107549170992 Score: 9\n",
      "Epoch: 1075 Avg Loss: 0.0103890046327 Score: 9\n",
      "Epoch: 1100 Avg Loss: 0.0100445748403 Score: 9\n",
      "Epoch: 1125 Avg Loss: 0.00971991296346 Score: 9\n",
      "Epoch: 1150 Avg Loss: 0.00941343856569 Score: 9\n",
      "Epoch: 1175 Avg Loss: 0.00912444740706 Score: 9\n",
      "Epoch: 1200 Avg Loss: 0.00885038086168 Score: 9\n",
      "Epoch: 1225 Avg Loss: 0.00859095294808 Score: 10\n",
      "Epoch: 1250 Avg Loss: 0.0083447115021 Score: 9\n",
      "Epoch: 1275 Avg Loss: 0.00811052600398 Score: 10\n",
      "Epoch: 1300 Avg Loss: 0.00788864979578 Score: 10\n",
      "Epoch: 1325 Avg Loss: 0.007677214395 Score: 10\n",
      "Epoch: 1350 Avg Loss: 0.00747545958916 Score: 10\n",
      "Epoch: 1375 Avg Loss: 0.00728319572037 Score: 10\n",
      "Epoch: 1400 Avg Loss: 0.00709963773196 Score: 10\n",
      "Epoch: 1425 Avg Loss: 0.00692443809797 Score: 10\n",
      "Epoch: 1450 Avg Loss: 0.00675703271925 Score: 10\n",
      "Epoch: 1475 Avg Loss: 0.00659657869331 Score: 10\n",
      "Epoch: 1500 Avg Loss: 0.00644322854644 Score: 10\n",
      "Epoch: 1525 Avg Loss: 0.00629602251805 Score: 10\n",
      "Epoch: 1550 Avg Loss: 0.00615485320561 Score: 10\n",
      "Epoch: 1575 Avg Loss: 0.00601941201885 Score: 10\n",
      "Epoch: 1600 Avg Loss: 0.00588935756042 Score: 10\n",
      "Epoch: 1625 Avg Loss: 0.00576407221873 Score: 10\n",
      "Epoch: 1650 Avg Loss: 0.00564387476399 Score: 10\n",
      "Epoch: 1675 Avg Loss: 0.00552817748669 Score: 10\n",
      "Epoch: 1700 Avg Loss: 0.00541651902645 Score: 10\n",
      "Epoch: 1725 Avg Loss: 0.00530934522706 Score: 10\n",
      "Epoch: 1750 Avg Loss: 0.00520570087338 Score: 10\n",
      "Epoch: 1775 Avg Loss: 0.00510596283748 Score: 10\n",
      "Epoch: 1800 Avg Loss: 0.00500955983748 Score: 10\n",
      "Epoch: 1825 Avg Loss: 0.00491647571962 Score: 10\n",
      "Epoch: 1850 Avg Loss: 0.00482638748928 Score: 10\n",
      "Epoch: 1875 Avg Loss: 0.00473941121257 Score: 10\n",
      "Epoch: 1900 Avg Loss: 0.00465527288201 Score: 10\n",
      "Epoch: 1925 Avg Loss: 0.00457375416659 Score: 10\n",
      "Epoch: 1950 Avg Loss: 0.00449506595073 Score: 10\n",
      "Epoch: 1975 Avg Loss: 0.00441863352006 Score: 10\n",
      "Epoch: 2000 Avg Loss: 0.00434466291334 Score: 10\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 10\n",
    "vocab_size = len(vocabulary)\n",
    "batch_size = 1024\n",
    "learning_rate = 1.0\n",
    "num_epochs = 2000\n",
    "alpha = 0.75\n",
    "xmax = 50\n",
    "\n",
    "glove = Glove(embedding_dim, vocab_size, batch_size)\n",
    "glove.init_weights()\n",
    "optimizer = torch.optim.Adadelta(glove.parameters(), lr=learning_rate)\n",
    "data_iter = batch_iter(nonzero_pairs, cooccurrences, batch_size)\n",
    "\n",
    "training_loop(batch_size, num_epochs, glove, optimizer, data_iter, xmax, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
